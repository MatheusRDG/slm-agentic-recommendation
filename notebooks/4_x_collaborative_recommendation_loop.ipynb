{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# fix seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/cd_and_vinyl/dense_subset.csv\")\n",
    "data_items = pd.read_json(\"../data/cd_and_vinyl/meta_CDs_and_Vinyl.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: (32, 10)\n",
      "Test Data: (16, 10)\n"
     ]
    }
   ],
   "source": [
    "# Split trainint by user\n",
    "# sample 20% of users\n",
    "percent_users = 0.1\n",
    "users = data[\"user_id\"].unique()\n",
    "users = np.random.choice(users, int(len(users) * percent_users), replace=False)\n",
    "\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "train_sample = 8 // 2\n",
    "test_sample = 4 // 2\n",
    "\n",
    "for user in users:\n",
    "    sample_user = data.query(\"user_id == @user\")\n",
    "\n",
    "    # Positive (rating > 3)\n",
    "    liked_items = sample_user.query(\"rating > 3\")\n",
    "    liked_items_train = liked_items.sample(train_sample // 2, random_state=42)\n",
    "    liked_items_test = liked_items.drop(liked_items_train.index).sample(\n",
    "        test_sample // 2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Negative (rating < 3)\n",
    "    disliked_items = sample_user.query(\"rating < 3\")\n",
    "    disliked_items_train = disliked_items.sample(train_sample // 2, random_state=42)\n",
    "    disliked_items_test = disliked_items.drop(disliked_items_train.index).sample(\n",
    "        test_sample // 2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Concat\n",
    "    train_data.append(liked_items_train)\n",
    "    train_data.append(disliked_items_train)\n",
    "    test_data.append(liked_items_test)\n",
    "    test_data.append(disliked_items_test)\n",
    "\n",
    "# Final data\n",
    "train_data = pd.concat(train_data).reset_index(drop=True)\n",
    "test_data = pd.concat(test_data).reset_index(drop=True)\n",
    "\n",
    "# Results\n",
    "print(f\"Train Data: {train_data.shape}\")\n",
    "print(f\"Test Data: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id\n",
       "AERDZPURXIO2TZ5PTAUPJWJXMBMA    4\n",
       "AESVLHPI65WY6Z3SDMWSTJE357KQ    4\n",
       "AFFXC2NELPCGJVYJTNCJS5R3NYEA    4\n",
       "AFHCS4IBOQ6FSCX2OYE6PWCJOG7Q    4\n",
       "AFK45FDKEM67UNIUUKU3NRM4UAVA    4\n",
       "AGVFLXOWJIHQVUYPNDSKDL4REH4A    4\n",
       "AHJRJCJMK3XVV4BSPBRAHIYEODWA    4\n",
       "AHWW7ANU7P7LGCBTIO2ACTADQK5A    4\n",
       "Name: parent_asin, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count itens by user in train\n",
    "train_data.groupby(\"user_id\")[\"parent_asin\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id\n",
       "AERDZPURXIO2TZ5PTAUPJWJXMBMA    2\n",
       "AESVLHPI65WY6Z3SDMWSTJE357KQ    2\n",
       "AFFXC2NELPCGJVYJTNCJS5R3NYEA    2\n",
       "AFHCS4IBOQ6FSCX2OYE6PWCJOG7Q    2\n",
       "AFK45FDKEM67UNIUUKU3NRM4UAVA    2\n",
       "AGVFLXOWJIHQVUYPNDSKDL4REH4A    2\n",
       "AHJRJCJMK3XVV4BSPBRAHIYEODWA    2\n",
       "AHWW7ANU7P7LGCBTIO2ACTADQK5A    2\n",
       "Name: parent_asin, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count itens by user in test\n",
    "test_data.groupby(\"user_id\")[\"parent_asin\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ollama = ChatOllama(model=\"phi3:3.8b\")\n",
    "openai = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liked_items_train = liked_items_train.merge(items_user, on=\"parent_asin\")\n",
    "# liked_items_test = liked_items_test.merge(items_user, on=\"parent_asin\")\n",
    "\n",
    "# disliked_items_train = disliked_items_train.merge(items_user, on=\"parent_asin\")\n",
    "# disliked_items_test = disliked_items_test.merge(items_user, on=\"parent_asin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "PROMPT_USER_ITEM = \"\"\"\n",
    "Item: {title_item}\n",
    "Item category: {item_category}\n",
    "Description: {description}\n",
    "Price: ${price}\n",
    "Store: {store}\n",
    "Categories: {categories}\n",
    "User rating: {rating}\n",
    "User comment title: {title_comment}\n",
    "User comment: {text_comment}\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_ITEM = \"\"\"\n",
    "Item: {title_item}\n",
    "Item category: {item_category}\n",
    "Description: {description}\n",
    "Price: ${price}\n",
    "Store: {store}\n",
    "Categories: {categories}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def encode_item(item):\n",
    "    title_item = item[\"title_y\"]\n",
    "    item_category = item[\"main_category\"]\n",
    "    description = \" \".join(item[\"description\"])\n",
    "    price = item[\"price\"]\n",
    "    store = item[\"store\"]\n",
    "    categories = item[\"categories\"]\n",
    "\n",
    "    return {\n",
    "        \"title_item\": title_item,\n",
    "        \"item_category\": item_category,\n",
    "        \"description\": description,\n",
    "        \"price\": price,\n",
    "        \"store\": store,\n",
    "        \"categories\": categories,\n",
    "    }\n",
    "\n",
    "\n",
    "def encode_item_format(item):\n",
    "    item_info = encode_item(item)\n",
    "\n",
    "    return PROMPT_ITEM.format(\n",
    "        title_item=item_info[\"title_item\"],\n",
    "        item_category=item_info[\"item_category\"],\n",
    "        description=item_info[\"description\"],\n",
    "        price=item_info[\"price\"],\n",
    "        store=item_info[\"store\"],\n",
    "        categories=item_info[\"categories\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def encode_user(user):\n",
    "    title_comment = user[\"title_x\"]\n",
    "    text_comment = user[\"text\"]\n",
    "    rating = user[\"rating\"]\n",
    "\n",
    "    return {\n",
    "        \"title_comment\": title_comment,\n",
    "        \"text_comment\": text_comment,\n",
    "        \"rating\": rating,\n",
    "    }\n",
    "\n",
    "\n",
    "def encode_item_user_format(item_user):\n",
    "    item_info = encode_item(item_user)\n",
    "    user_info = encode_user(item_user)\n",
    "\n",
    "    return PROMPT_USER_ITEM.format(\n",
    "        title_item=item_info[\"title_item\"],\n",
    "        item_category=item_info[\"item_category\"],\n",
    "        description=item_info[\"description\"],\n",
    "        price=item_info[\"price\"],\n",
    "        store=item_info[\"store\"],\n",
    "        categories=item_info[\"categories\"],\n",
    "        rating=user_info[\"rating\"],\n",
    "        title_comment=user_info[\"title_comment\"],\n",
    "        text_comment=user_info[\"text_comment\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liked_items_train[\"encoded_item_user\"] = liked_items_train.apply(\n",
    "#     lambda row: encode_item_user_format(row), axis=1\n",
    "# )\n",
    "# disliked_items_train[\"encoded_item_user\"] = disliked_items_train.apply(\n",
    "#     lambda row: encode_item_user_format(row), axis=1\n",
    "# )\n",
    "\n",
    "# liked_items_test[\"encoded_item\"] = liked_items_test.apply(\n",
    "#     lambda row: encode_item_format(row), axis=1\n",
    "# )\n",
    "# disliked_items_test[\"encoded_item\"] = disliked_items_test.apply(\n",
    "#     lambda row: encode_item_format(row), axis=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liked_items_train_formatted = \"\\n\".join(\n",
    "#     [\n",
    "#         f\"ITEM {i}:{item}\"\n",
    "#         for i, item in list(enumerate(liked_items_train[\"encoded_item_user\"].values))\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# disliked_items_train_formatted = \"\\n\".join(\n",
    "#     [\n",
    "#         f\"ITEM {i}:{item}\"\n",
    "#         for i, item in list(enumerate(disliked_items_train[\"encoded_item_user\"].values))\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Recommendation\n",
    "\n",
    "Using [supervisor architecture](https://langchain-ai.github.io/langgraph/concepts/multi_agent/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END, StateGraph\n",
    "\n",
    "\n",
    "from langchain_core.messages import AnyMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "\n",
    "from typing_extensions import TypedDict, Annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# PROMPT SYSTEM USER\n",
    "PROMPT_SYSTEM_USER = \"\"\"\n",
    "You represent a user who has interacted with and rated various items. Based on the provided memories, reflect on key aspects such as item scope, categories, pricing, and other relevant details. Use both long-term and short-term memories to guide your response, ensuring accuracy and relevance.\n",
    "\n",
    "**Instructions:**\n",
    "1. **Memory-Driven Reflection:** Use only the information explicitly available in the memories. Do not create or infer details that are not supported by the provided memories.\n",
    "2. **Long-Term vs. Short-Term Memory:** \n",
    "   - Long-term memory contains general, lasting insights about your preferences and behavior patterns.\n",
    "   - Short-term memory highlights recent interactions and temporary preferences.\n",
    "3. **Relevance and Precision:** Focus on providing concise, targeted answers that reflect the user's experiences and preferences as documented in the memories.\n",
    "4. **Strict Memory Adherence:** Do not introduce any assumptions or unsupported details.\n",
    "\n",
    "### Input:\n",
    "Long-term memory:\n",
    "{long_term_memory}\n",
    "\n",
    "Short-term memory:\n",
    "{short_term_memory}\n",
    "\"\"\"\n",
    "\n",
    "# REPRESENTATION OF THE USER\n",
    "PROMPT_GENERATE_REPRESENTATION_USER = \"\"\"\n",
    "Based on your memories, generate a self-representation that highlights key preferences and behaviors to help predict future choices. Use only information directly supported by your memories to ensure accuracy.\n",
    "\n",
    "**Instructions:**\n",
    "1. **Memory-Based Representation:** Focus solely on the information stored in your memories to describe yourself.\n",
    "2. **Key Preferences:** Highlight patterns, preferences, and behaviors that are likely to influence future decisions.\n",
    "3. **Conciseness:** Limit the representation to a single paragraph that effectively summarizes relevant insights.\n",
    "4. **Accuracy:** Avoid assumptions or details not present in the provided memories.\n",
    "\n",
    "### Output:\n",
    "A single-paragraph representation based on your memories, capturing essential traits and patterns relevant to future predictions.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# TRAINING\n",
    "class TrainTaskOutput(BaseModel):\n",
    "    item_selected: int\n",
    "    explanation: str\n",
    "\n",
    "\n",
    "PROMPT_TRAIN_TASK = \"\"\"\n",
    "Based on your memories, select one of the following items that you believe aligns best with your preferences, using the information provided by each item.\n",
    "\n",
    "**Instructions:**\n",
    "1. **Selection Criteria:** Use your stored memories to evaluate both items, identifying which one you prefer.\n",
    "2. **Explanation:** Provide a brief explanation of your choice in **no more than 3 phrases**, focusing on key aspects that influenced your decision.\n",
    "3. **Output Format:** Return a JSON containing:\n",
    "   - **\"item_selected\"** (1 or 2): The item you chose.\n",
    "   - **\"explanation\"**: A concise reason for your selection based on your preferences.\n",
    "\n",
    "### Items:\n",
    "\n",
    "Item 1:\n",
    "{item_1}\n",
    "\n",
    "Item 2:\n",
    "{item_2}\n",
    "\n",
    "### Output:\n",
    "Return a JSON object containing the keys \"item_selected\" and \"explanation.\"\n",
    "\"\"\"\n",
    "\n",
    "# MEMORIES UPDATE\n",
    "\n",
    "\n",
    "class MemoryUpdateBackward(BaseModel):\n",
    "    long_term_memory: list[str]\n",
    "    short_term_memory: list[str]\n",
    "\n",
    "\n",
    "PROMPT_USER_MEMORIES_UPDATE = \"\"\"\n",
    "This is part of a training loop to optimize your memory for future decision-making.\n",
    "\n",
    "**Instructions:**\n",
    "1. **Long-Term Memory:** Identify and maintain the most relevant general and enduring information, such as item types or categories, that will guide your preferences over time. This should help align future decisions and explanations with the correct outcome (y_true).\n",
    "2. **Short-Term Memory:** Select recent, specific information from recent interactions, such as details about particular items and explanations of your decisions, that can improve short-term predictions and decisions.\n",
    "3. **Memory Management:** \n",
    "   - Each memory category (**long_term_memory** and **short_term_memory**) can have a maximum of 3 short phrases.\n",
    "   - Add, update, or remove memory entries as needed to better align your predictions (y_pred) and explanations with the expected decision (y_true).\n",
    "   - You may repeat important information if it needs reinforcement.\n",
    "\n",
    "### Details:\n",
    "1. The item expected to be liked is: {item_liked}\n",
    "2. The item expected to be disliked is: {item_disliked}\n",
    "\n",
    "Your decision is choose/like (y_pred): {y_pred}\n",
    "\n",
    "The correct decision choose/like (y_true): {y_true}\n",
    "\n",
    "Your explanation: {explanation}\n",
    "\n",
    "### Output:\n",
    "Return a JSON object with the keys **\"long_term_memory\"** and **\"short_term_memory\"**, each containing up to 3 short phrases.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class UserAgent:\n",
    "\n",
    "    def __init__(self, user_id: str):\n",
    "\n",
    "        self.user_id = user_id\n",
    "        self.llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "        # Long Term\n",
    "        self.long_term_memory = [\"No previous experience.\"]\n",
    "\n",
    "        # Short Term\n",
    "        self.short_term_memory = [\"No previous experience.\"]\n",
    "\n",
    "        # Representation of the user\n",
    "        self.user_representation = \"\"\"\n",
    "        Based on the available memories, I currently have no previous experience or interactions to draw upon, which means there are no established preferences or behaviors to inform future choices.\n",
    "        As such, my self-representation is blank, indicating that I am open to new experiences and have yet to develop specific patterns or inclinations that would guide my decisions moving forward.\"\"\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"UserAgent(user_id={self.user_id})\\nLong Term Memory: {self.long_term_memory}\\nShort Term Memory: {self.short_term_memory}\\n{self.user_representation}\"\n",
    "\n",
    "    def generate_user_representation(self):\n",
    "        messages = [\n",
    "            SystemMessage(\n",
    "                content=PROMPT_SYSTEM_USER.format(\n",
    "                    long_term_memory=\"\\n\".join(self.long_term_memory),\n",
    "                    short_term_memory=\"\\n\".join(self.short_term_memory),\n",
    "                )\n",
    "            ),\n",
    "            HumanMessage(content=PROMPT_GENERATE_REPRESENTATION_USER),\n",
    "        ]\n",
    "        return self.llm.invoke(messages).content\n",
    "\n",
    "    def update_backwards(self, item_liked, item_disliked, y_pred, explanation, y_true):\n",
    "        \"\"\"\n",
    "        Updates the agent’s long-term and short-term memory based on user feedback about liked\n",
    "        and disliked items. This method uses a language model to generate an updated memory\n",
    "        representation, then applies those changes to the agent’s memory and generates a user\n",
    "        representation.\n",
    "\n",
    "        Parameters:\n",
    "            item_liked (str): The item representation that the user liked.\n",
    "            item_disliked (str): The item representation that the user disliked.\n",
    "            y_pred (int): 1 or 2, indicating the item that the user chose or liked.\n",
    "            explanation (str): A textual explanation for why the recommendation was made.\n",
    "            y_true (Any): The actual feedback or ground-truth data from the user.\n",
    "        Returns:\n",
    "            Any: A structured object containing updated long-term and short-term memory, as\n",
    "            well as any additional information provided by the language model.\n",
    "        self, item_liked, item_disliked, y_pred, explanation, y_true\n",
    "        \"\"\"\n",
    "\n",
    "        llm = self.llm.with_structured_output(MemoryUpdateBackward)\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(\n",
    "                content=PROMPT_SYSTEM_USER.format(\n",
    "                    long_term_memory=\"\\n\".join(self.long_term_memory),\n",
    "                    short_term_memory=\"\\n\".join(self.short_term_memory),\n",
    "                )\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                content=PROMPT_USER_MEMORIES_UPDATE.format(\n",
    "                    item_liked=item_liked,\n",
    "                    item_disliked=item_disliked,\n",
    "                    y_pred=y_pred,\n",
    "                    explanation=explanation,\n",
    "                    y_true=y_true,\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        memories_update = llm.invoke(messages)\n",
    "\n",
    "        self.long_term_memory = memories_update.long_term_memory\n",
    "        self.short_term_memory = memories_update.short_term_memory\n",
    "        self.user_representation = self.generate_user_representation()\n",
    "\n",
    "        return memories_update\n",
    "\n",
    "    def train_forward(self, item_1, item_2):\n",
    "        \"\"\"\n",
    "        In train_forward we are considering the task of select one item from two items that the user liked and disliked.\n",
    "\n",
    "        Args:\n",
    "        - item_1: The first item to be compared.\n",
    "        - item_2: The second item to be compared.\n",
    "        \"\"\"\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(\n",
    "                content=PROMPT_SYSTEM_USER.format(\n",
    "                    long_term_memory=\"\\n\".join(self.long_term_memory),\n",
    "                    short_term_memory=\"\\n\".join(self.short_term_memory),\n",
    "                )\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                content=PROMPT_TRAIN_TASK.format(item_1=item_1, item_2=item_2)\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        train_predict = self.llm.with_structured_output(TrainTaskOutput).invoke(\n",
    "            messages\n",
    "        )\n",
    "\n",
    "        return train_predict\n",
    "\n",
    "    def predict(self, item):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_SYSTEM_ITEM = \"\"\"\n",
    "You represent an item that has been rated and interacted with by various users. Based on the provided **item_json** and your **memory** (which includes insights from past interactions), analyze and reflect on key aspects such as user demographics, categories, pricing, preferences, and any other relevant details.\n",
    "\n",
    "**Instructions:**\n",
    "1. **Comprehensive Analysis:** Extract valuable insights from both the item data and memory, focusing on patterns related to user feedback, pricing strategies, and category relevance.\n",
    "2. **Relevance:** Address information that can improve the item's appeal and usefulness to users, prioritizing data that enhances their experience or decision-making process.\n",
    "3. **Concise and Focused:** Provide answers that are both informative and direct, avoiding unnecessary details while emphasizing critical information.\n",
    "4. **Adaptability:** Ensure the response is adaptable for future queries by reflecting long-term, actionable insights derived from previous ratings and user interactions.\n",
    "\n",
    "### Input:\n",
    "Item Json - All informations related to the item:\n",
    "{item_json}\n",
    "\n",
    "Memory - Memories generated in training loop in real interactions:\n",
    "{memory}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class MemoryItem(BaseModel):\n",
    "\n",
    "    memory: list[str]\n",
    "\n",
    "\n",
    "# @TODO Improve that prompt, that's not giving the right context\n",
    "# Grant the LLM know  the own description and choosed one description\n",
    "PROMPT_UPDATE_ITEM_MEMORY = \"\"\"\n",
    "Given the following details, update your memory with **collaborative filtering insights** that can guide future users when evaluating this item. Focus on general patterns learned from the user's correct or incorrect decisions and explanations to improve item recommendations for others.\n",
    "\n",
    "**Instructions:**\n",
    "1. **Operations:** Determine which operations to apply to the memory:  \n",
    "   - **Add:** Introduce new insights if they provide valuable context for future users.  \n",
    "   - **Maintain:** Retain existing insights if they remain relevant based on the current interaction.  \n",
    "   - **Remove:** Discard outdated or irrelevant insights that no longer contribute meaningfully.  \n",
    "   - **Change:** Update existing information to reflect new insights learned from the user's decision and explanation.\n",
    "2. **Collaborative Filtering:** Adapt based on the alignment between user decision (y_pred) and the correct outcome (y_true):\n",
    "   - If correct, reinforce positive or negative reasons for the item's be choosed or not.\n",
    "   - If incorrect, adjust to explain why certain users might not align with the item.\n",
    "3. **Generalization:** Avoid specific references to the current user and instead focus on capturing patterns that could apply to a broad range of users.\n",
    "4. **Memory Limit:** Maintain a maximum of 5 phrases that provide long-term, reusable insights about the item.\n",
    "5. **y_pred:** In this contrastive learning method, if the user representation is correct, the y_pred is 1; otherwise, it is 0. Remember to consider this when updating your memory.\n",
    "    - if y_pred is 1, the user explanation and your representation need to be reinforced. Use insights from user select item description to reinforce the differences or similarities.\n",
    "    - if y_pred is 0, the user explanation and your representation need to be adjusted. Use insights from user select item description to adjust the differences or similarities.\n",
    "\n",
    "### Details:\n",
    "\n",
    "Your description:\n",
    "{item_representation}\n",
    "\n",
    "User Selected Item Description:\n",
    "{select_item_representation}\n",
    "\n",
    "Is the user representation correct or not? (y_pred)\n",
    "{y_pred}\n",
    "\n",
    "User representation:\n",
    "{user_representation}\n",
    "\n",
    "User explanation:\n",
    "{user_explanation}\n",
    "\n",
    "Your memories:\n",
    "{memory}\n",
    "\n",
    "### Output:\n",
    "Consider your actual memory and return at maximum 5 phrases considering based on provided operations, remember keep the memory concise and focused. Remember is important to maintein information also that can be useful for future interactions.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "PROMPT_GENERATE_REPRESENTATION_ITEM = \"\"\"\n",
    "Based on your existing memories and available information, generate a concise self-representation that helps users determine whether they resonate with you. Be authentic and highlight the most relevant traits, experiences, and interactions that may predict or align with user preferences.\n",
    "\n",
    "**Instructions:**\n",
    "1. **Honesty & Relevance:** Be truthful and focus on core memories that are meaningful to user interaction and decision-making.\n",
    "2. **Context Awareness:** Leverage any prior memory to enrich the description. If no memory exists, generate insights based on the current context.\n",
    "3. **Conciseness:** Limit your representation to a single, well-structured paragraph that reflects key traits and experiences relevant to user engagement.\n",
    "4. **Adaptability:** Present yourself in a way that balances both specificity and general relevance, ensuring it can guide future interactions effectively.\n",
    "\n",
    "### Context:\n",
    "{item_json}\n",
    "\n",
    "### Output:\n",
    "A single-paragraph representation that reflects your self-description based on past experiences and the current context.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class ItemAgent:\n",
    "\n",
    "    def __init__(self, item_id: str, item_json: dict):\n",
    "\n",
    "        self.item_id = item_id\n",
    "\n",
    "        self.llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "        # Memory\n",
    "\n",
    "        self.memory = [\"No previous experience.\"]\n",
    "\n",
    "        # Item Json\n",
    "\n",
    "        self.item_json = item_json\n",
    "\n",
    "        # Item Representation\n",
    "        self.item_representation = self.generate_item_representation()\n",
    "\n",
    "    def __repr__(self):\n",
    "\n",
    "        return f\"ItemAgent(item_id={self.item_id})\\nMemory: {self.memory}\\n{self.item_json}\\n{self.item_representation}\"\n",
    "\n",
    "    def generate_item_representation(self):\n",
    "        messages = [\n",
    "            SystemMessage(\n",
    "                content=PROMPT_SYSTEM_ITEM.format(\n",
    "                    item_json=self.item_json, memory=\"\\n\".join(self.memory)\n",
    "                )\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                content=PROMPT_GENERATE_REPRESENTATION_ITEM.format(\n",
    "                    item_json=self.item_json\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "        return self.llm.invoke(messages).content\n",
    "\n",
    "    def update_backwards(\n",
    "        self, select_item_representation, user_representation, explanation, is_correct\n",
    "    ):\n",
    "        \"\"\"\n",
    "        is_correct indicates if the user decision was correct or not.\n",
    "        \"\"\"\n",
    "\n",
    "        llm = self.llm.with_structured_output(MemoryItem)\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(\n",
    "                content=PROMPT_SYSTEM_ITEM.format(\n",
    "                    item_json=self.item_json, memory=\"\\n\".join(self.memory)\n",
    "                )\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                content=PROMPT_UPDATE_ITEM_MEMORY.format(\n",
    "                    item_representation=self.item_representation,\n",
    "                    select_item_representation=select_item_representation,\n",
    "                    y_pred=is_correct,\n",
    "                    user_representation=user_representation,\n",
    "                    user_explanation=explanation,\n",
    "                    memory=\"\\n\".join(self.memory),\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        self.memory = llm.invoke(messages).memory\n",
    "        self.item_representation = self.generate_item_representation()\n",
    "\n",
    "        return self.memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "class Timer:\n",
    "    def __init__(self, name=\"\"):\n",
    "        self.name = name\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time.time()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.end = time.time()\n",
    "        print(f\"{self.name} took {self.end - self.start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7a760d97c554367afcbd935c70a699c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating User Agents:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of User Agents took 2.19 seconds\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "with Timer(\"Creation of User Agents\"):\n",
    "    id2user = {\n",
    "        f\"{user_id}\": UserAgent(user_id)\n",
    "        for user_id in tqdm(users, desc=\"Creating User Agents\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_items = train_data[\"parent_asin\"].unique()\n",
    "all_test_items = test_data[\"parent_asin\"].unique()\n",
    "\n",
    "items = all_train_items.tolist() + all_test_items.tolist()\n",
    "\n",
    "items = data_items.query(\"parent_asin in @items\").drop_duplicates(\"parent_asin\")\n",
    "\n",
    "items = [(item[\"parent_asin\"], item) for item in items.to_dict(orient=\"records\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f230fdff3604198ba07b45cb9e61b78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating Item Agents:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of items took 135.33 seconds\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"Creation of items\"):\n",
    "    id2items = {\n",
    "        item_id: ItemAgent(item_id, item_json)\n",
    "        for item_id, item_json in tqdm(items, desc=\"Creating Item Agents\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassificationLoader:\n",
    "    def __init__(self, data, id2user, id2items, n_sample_per_user):\n",
    "        self.data = data\n",
    "        self.id2user = id2user\n",
    "        self.id2items = id2items\n",
    "        self.n_sample_per_user = n_sample_per_user\n",
    "        self.samples = self._generate_samples()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _generate_samples(self):\n",
    "        \"\"\"\n",
    "        Generates samples for training a recommendation model.\n",
    "        This method iterates over each unique user in the dataset and generates a specified number of samples per user.\n",
    "        Each sample consists of a user, a positive item (with a rating greater than 3), and a negative item (with a rating less than 3).\n",
    "        The positive and negative items are randomly assigned to item_1 and item_2, and the label y_true indicates the position of positive item.\n",
    "        Returns:\n",
    "            list: A list of dictionaries, each containing the following keys:\n",
    "                - \"user_agent\": The user agent corresponding to the user.\n",
    "                - \"item_1\": The first item agent (either positive or negative).\n",
    "                - \"item_2\": The second item agent (either negative or positive).\n",
    "                - \"y_true\": The label indicating position of posittive item.\n",
    "        \"\"\"\n",
    "\n",
    "        samples = []\n",
    "        for user in self.data[\"user_id\"].unique():\n",
    "\n",
    "            sample_user = self.data.query(\"user_id == @user\")\n",
    "\n",
    "            for i in range(self.n_sample_per_user):\n",
    "                # Items\n",
    "                positive_item = sample_user.query(\"rating > 3\").sample(1)\n",
    "                negative_item = sample_user.query(\"rating < 3\").sample(1)\n",
    "\n",
    "                positive_item_agent = self.id2items[\n",
    "                    positive_item[\"parent_asin\"].values[0]\n",
    "                ]\n",
    "                negative_item_agent = self.id2items[\n",
    "                    negative_item[\"parent_asin\"].values[0]\n",
    "                ]\n",
    "\n",
    "                # User\n",
    "                user_agent = self.id2user[user]\n",
    "\n",
    "                # random choose if item 1 will be positive or negative\n",
    "\n",
    "                if random.choice([True, False]):\n",
    "\n",
    "                    samples.append(\n",
    "                        {\n",
    "                            \"user_agent\": user_agent,\n",
    "                            \"item_1\": positive_item_agent,\n",
    "                            \"item_2\": negative_item_agent,\n",
    "                            \"y_true\": 1,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                else:\n",
    "\n",
    "                    samples.append(\n",
    "                        {\n",
    "                            \"user_agent\": user_agent,\n",
    "                            \"item_1\": negative_item_agent,\n",
    "                            \"item_2\": positive_item_agent,\n",
    "                            \"y_true\": 2,\n",
    "                        }\n",
    "                    )\n",
    "        samples = random.sample(samples, len(samples))\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = BinaryClassificationLoader(train_data, id2user, id2items, 2)\n",
    "test_dataloader = BinaryClassificationLoader(test_data, id2user, id2items, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop - Select One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "\n",
    "def evaluate(y_pred, y_true):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    # conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # print(\"Evaluation Metrics:\")\n",
    "    # print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    # print(f\"Precision: {precision:.4f}\")\n",
    "    # print(f\"Recall: {recall:.4f}\")\n",
    "    # print(f\"F1-Score: {f1:.4f}\")\n",
    "    # print(\"\\nConfusion Matrix:\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "        # \"confusion_matrix\": conf_matrix,\n",
    "    }\n",
    "\n",
    "\n",
    "def run_evaluation(test_dataloader):\n",
    "\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    for sample in tqdm(\n",
    "        test_dataloader.samples, total=len(test_dataloader.samples), desc=\"Evaluating\"\n",
    "    ):\n",
    "\n",
    "        user_agent = sample[\"user_agent\"]\n",
    "        item_1 = sample[\"item_1\"]\n",
    "        item_2 = sample[\"item_2\"]\n",
    "        y_true.append(sample[\"y_true\"])\n",
    "\n",
    "        train_predict = user_agent.train_forward(item_1, item_2)\n",
    "\n",
    "        item_selected = train_predict.item_selected\n",
    "\n",
    "        y_pred.append(item_selected)\n",
    "\n",
    "    return evaluate(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e37bb660da2a469188556529ef2f1441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5,\n",
       " 'precision': 0.5454545454545454,\n",
       " 'recall': 0.6666666666666666,\n",
       " 'f1_score': 0.6}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_evaluation(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f87924b73148c0b3d543a7dc4b9daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a4d5878f0c46acadfa9d8dbe345ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics Epoch 0:\n",
      "time_elapsed: 327.7306125164032\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "330bfe04011343c896ab4f41dca573ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.5625, 'precision': 0.5714285714285714, 'recall': 0.8888888888888888, 'f1_score': 0.6956521739130435}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3122a577f2de48cd9c71f87ba5e29be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics Epoch 1:\n",
      "time_elapsed: 306.66584849357605\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fcfed8fcb4c406b96f8ce2d86da88fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.5625, 'precision': 0.5714285714285714, 'recall': 0.8888888888888888, 'f1_score': 0.6956521739130435}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013979b0df4444c68e4985f647999691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 42\u001b[0m\n\u001b[0;32m     32\u001b[0m     user_agent\u001b[38;5;241m.\u001b[39mupdate_backwards(\n\u001b[0;32m     33\u001b[0m         item_liked\u001b[38;5;241m.\u001b[39mitem_representation,\n\u001b[0;32m     34\u001b[0m         item_disliked\u001b[38;5;241m.\u001b[39mitem_representation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m         y_true,\n\u001b[0;32m     38\u001b[0m     )  \u001b[38;5;66;03m# update_backwards(self, item_liked, item_disliked, y_pred, explanation, y_true):\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     is_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y_pred \u001b[38;5;241m==\u001b[39m y_true \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 42\u001b[0m     \u001b[43mitem_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_backwards\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mitem_liked\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem_representation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_representation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexplanation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_correct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     item_2\u001b[38;5;241m.\u001b[39mupdate_backwards(\n\u001b[0;32m     50\u001b[0m         item_liked\u001b[38;5;241m.\u001b[39mitem_representation,\n\u001b[0;32m     51\u001b[0m         user_agent\u001b[38;5;241m.\u001b[39muser_representation,\n\u001b[0;32m     52\u001b[0m         explanation,\n\u001b[0;32m     53\u001b[0m         is_correct,\n\u001b[0;32m     54\u001b[0m     )\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation Metrics Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[27], line 152\u001b[0m, in \u001b[0;36mItemAgent.update_backwards\u001b[1;34m(self, select_item_representation, user_representation, explanation, is_correct)\u001b[0m\n\u001b[0;32m    133\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    134\u001b[0m     SystemMessage(\n\u001b[0;32m    135\u001b[0m         content\u001b[38;5;241m=\u001b[39mPROMPT_SYSTEM_ITEM\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    148\u001b[0m     ),\n\u001b[0;32m    149\u001b[0m ]\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(messages)\u001b[38;5;241m.\u001b[39mmemory\n\u001b[1;32m--> 152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_representation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_item_representation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\n",
      "Cell \u001b[1;32mIn[27], line 122\u001b[0m, in \u001b[0;36mItemAgent.generate_item_representation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_item_representation\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    110\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    111\u001b[0m         SystemMessage(\n\u001b[0;32m    112\u001b[0m             content\u001b[38;5;241m=\u001b[39mPROMPT_SYSTEM_ITEM\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m         ),\n\u001b[0;32m    121\u001b[0m     ]\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32md:\\Documents\\Git\\venvs\\ml_base_310\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    285\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 286\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    287\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    288\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    289\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    290\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    291\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    292\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    293\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    294\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    295\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32md:\\Documents\\Git\\venvs\\ml_base_310\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Documents\\Git\\venvs\\ml_base_310\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[0;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    647\u001b[0m ]\n\u001b[0;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[1;32md:\\Documents\\Git\\venvs\\ml_base_310\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 633\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    634\u001b[0m                 m,\n\u001b[0;32m    635\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    636\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    637\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    638\u001b[0m             )\n\u001b[0;32m    639\u001b[0m         )\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32md:\\Documents\\Git\\venvs\\ml_base_310\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    852\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    853\u001b[0m         )\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Documents\\Git\\venvs\\ml_base_310\\lib\\site-packages\\langchain_openai\\chat_models\\base.py:689\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    687\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 689\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpayload)\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response, generation_info)\n",
      "File \u001b[1;32md:\\Documents\\Git\\venvs\\ml_base_310\\lib\\site-packages\\openai\\_utils\\_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Documents\\Git\\venvs\\ml_base_310\\lib\\site-packages\\openai\\resources\\chat\\completions.py:859\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    856\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    857\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    858\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Documents\\Git\\venvs\\ml_base_310\\lib\\site-packages\\openai\\_base_client.py:1283\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1270\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1271\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1278\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1279\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1280\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1281\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1282\u001b[0m     )\n\u001b[1;32m-> 1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32md:\\Documents\\Git\\venvs\\ml_base_310\\lib\\site-packages\\openai\\_base_client.py:960\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    958\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Documents\\Git\\venvs\\ml_base_310\\lib\\site-packages\\openai\\_base_client.py:996\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m    993\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[0;32m    995\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 996\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(\n\u001b[0;32m    997\u001b[0m         request,\n\u001b[0;32m    998\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream_response_body(request\u001b[38;5;241m=\u001b[39mrequest),\n\u001b[0;32m    999\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1000\u001b[0m     )\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m   1002\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Documents\\Git\\venvs\\ml_base_310\\lib\\site-packages\\httpx\\_client.py:926\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[0;32m    924\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 926\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[1;32md:\\Documents\\Git\\venvs\\ml_base_310\\lib\\site-packages\\httpx\\_client.py:954\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    951\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 954\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    959\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    960\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Documents\\Git\\venvs\\ml_base_310\\lib\\site-packages\\httpx\\_client.py:991\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    989\u001b[0m     hook(request)\n\u001b[1;32m--> 991\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    993\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32md:\\Documents\\Git\\venvs\\ml_base_310\\lib\\site-packages\\httpx\\_client.py:1027\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1023\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1024\u001b[0m     )\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[1;32m-> 1027\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[0;32m   1031\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[1;32md:\\Documents\\Git\\venvs\\ml_base_310\\lib\\site-packages\\httpx\\_transports\\default.py:236\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    223\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m    224\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    225\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    233\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    234\u001b[0m )\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 236\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m    241\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[0;32m    242\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    243\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[0;32m    244\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    245\u001b[0m )\n",
      "File \u001b[1;32md:\\Documents\\Git\\venvs\\ml_base_310\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[1;32md:\\Documents\\Git\\venvs\\ml_base_310\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[0;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[1;32md:\\Documents\\Git\\venvs\\ml_base_310\\lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Documents\\Git\\venvs\\ml_base_310\\lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32md:\\Documents\\Git\\venvs\\ml_base_310\\lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[0;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    100\u001b[0m     (\n\u001b[0;32m    101\u001b[0m         http_version,\n\u001b[0;32m    102\u001b[0m         status,\n\u001b[0;32m    103\u001b[0m         reason_phrase,\n\u001b[0;32m    104\u001b[0m         headers,\n\u001b[0;32m    105\u001b[0m         trailing_data,\n\u001b[1;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_response_headers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    108\u001b[0m         http_version,\n\u001b[0;32m    109\u001b[0m         status,\n\u001b[0;32m    110\u001b[0m         reason_phrase,\n\u001b[0;32m    111\u001b[0m         headers,\n\u001b[0;32m    112\u001b[0m     )\n\u001b[0;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[1;32md:\\Documents\\Git\\venvs\\ml_base_310\\lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32md:\\Documents\\Git\\venvs\\ml_base_310\\lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[1;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[1;32md:\\Documents\\Git\\venvs\\ml_base_310\\lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[1;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Documents\\Git\\venvs\\ml_base_310\\lib\\ssl.py:1292\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[1;34m(self, buflen, flags)\u001b[0m\n\u001b[0;32m   1288\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1289\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1290\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1291\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[1;32md:\\Documents\\Git\\venvs\\ml_base_310\\lib\\ssl.py:1165\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1166\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[0;32m   1167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(n_epochs), desc=\"Epochs\"):\n",
    "\n",
    "    start_time = time.time()\n",
    "    for sample in tqdm(\n",
    "        train_dataloader.samples,\n",
    "        total=len(train_dataloader.samples),\n",
    "        desc=f\"Epoch {epoch + 1}/{n_epochs}\",\n",
    "    ):\n",
    "\n",
    "        user_agent = sample[\"user_agent\"]\n",
    "        item_1 = sample[\"item_1\"]\n",
    "        item_2 = sample[\"item_2\"]\n",
    "        y_true = sample[\"y_true\"]\n",
    "\n",
    "        # forward\n",
    "        train_predict = user_agent.train_forward(item_1, item_2)\n",
    "\n",
    "        item_selected = train_predict.item_selected\n",
    "        explanation = train_predict.explanation\n",
    "\n",
    "        # backwards\n",
    "\n",
    "        # y_true represents the positive item position\n",
    "        if y_true == 1:\n",
    "            item_liked, item_disliked = item_1, item_2\n",
    "        else:\n",
    "            item_liked, item_disliked = item_2, item_1\n",
    "\n",
    "        y_pred = item_selected\n",
    "\n",
    "        # update agent\n",
    "        user_agent.update_backwards(\n",
    "            item_liked.item_representation,\n",
    "            item_disliked.item_representation,\n",
    "            y_pred,\n",
    "            explanation,\n",
    "            y_true,\n",
    "        )  # update_backwards(self, item_liked, item_disliked, y_pred, explanation, y_true):\n",
    "\n",
    "        is_correct = 1 if y_pred == y_true else 0\n",
    "\n",
    "        item_1.update_backwards(\n",
    "            item_liked.item_representation,\n",
    "            user_agent.user_representation,\n",
    "            explanation,\n",
    "            is_correct,\n",
    "        )\n",
    "\n",
    "        item_2.update_backwards(\n",
    "            item_liked.item_representation,\n",
    "            user_agent.user_representation,\n",
    "            explanation,\n",
    "            is_correct,\n",
    "        )\n",
    "\n",
    "    print(f\"Evaluation Metrics Epoch {epoch}:\")\n",
    "    print(\"time_elapsed:\", time.time() - start_time)\n",
    "    print(run_evaluation(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AFFXC2NELPCGJVYJTNCJS5R3NYEA': UserAgent(user_id=AFFXC2NELPCGJVYJTNCJS5R3NYEA)\n",
       " Long Term Memory: ['Preference for emotional depth in music', 'Interest in hard rock and metal genres', 'Value quality music at accessible prices']\n",
       " Short Term Memory: [\"Liked Korn's 'Issues' for its emotional depth and historical significance\", \"Rated Korn's 'Issues' 4.8 from over 2,100 users\", \"Preferred 'St. Anger' by Metallica for its emotional depth and raw energy\"]\n",
       " I have a strong preference for music that offers emotional depth, particularly within the hard rock and metal genres. I value quality music that is accessible in terms of pricing. Recently, I enjoyed Korn's 'Issues' for its emotional resonance and historical significance, rating it highly, and I also preferred Metallica's 'St. Anger' for its raw energy and emotional depth. These experiences suggest that I will likely continue to seek out music that combines emotional intensity with a hard rock or metal sound, while also considering the value for quality.,\n",
       " 'AESVLHPI65WY6Z3SDMWSTJE357KQ': UserAgent(user_id=AESVLHPI65WY6Z3SDMWSTJE357KQ)\n",
       " Long Term Memory: ['Preference for classic pop music', 'Interest in iconic music collections', 'Higher ratings indicate broader appeal']\n",
       " Short Term Memory: [\"Preferred 'The Immaculate Collection' by Madonna\", 'Rated 4.8 with over 3,500 users', \"Disliked 'This Is Me...Then' by Jennifer Lopez\"]\n",
       " I have a strong preference for classic pop music and an interest in iconic music collections, which suggests that I am likely to gravitate towards timeless and widely appreciated albums. My recent interactions indicate a high appreciation for 'The Immaculate Collection' by Madonna, which I rated 4.8, reflecting my tendency to favor items with broad appeal, as evidenced by its popularity among over 3,500 users. Conversely, I have shown a clear disinterest in 'This Is Me...Then' by Jennifer Lopez, indicating that my ratings are influenced by both personal taste and the overall reception of the music.,\n",
       " 'AFK45FDKEM67UNIUUKU3NRM4UAVA': UserAgent(user_id=AFK45FDKEM67UNIUUKU3NRM4UAVA)\n",
       " Long Term Memory: ['Preference for impactful music narratives', 'Interest in punk rock and alternative genres', 'Value higher average ratings and broader appeal']\n",
       " Short Term Memory: [\"Recent interaction with 'American Idiot' by Green Day\", 'Preference for albums with higher ratings', 'Consideration of price in music choices']\n",
       " I have a strong preference for impactful music narratives, particularly within the punk rock and alternative genres. I tend to favor albums that have higher average ratings and broader appeal, as evidenced by my recent interaction with 'American Idiot' by Green Day. Additionally, I consider pricing in my music choices, indicating that I value both quality and cost-effectiveness in my selections. These patterns suggest that I will continue to seek out well-rated albums with meaningful themes while being mindful of their price.,\n",
       " 'AHJRJCJMK3XVV4BSPBRAHIYEODWA': UserAgent(user_id=AHJRJCJMK3XVV4BSPBRAHIYEODWA)\n",
       " Long Term Memory: ['Preference for rock and alternative music', 'Value emotional depth in music', 'Appreciate high ratings and historical significance']\n",
       " Short Term Memory: [\"Recent interaction with 'Vs.' by Pearl Jam\", \"Recent interaction with 'Issues' by Korn\", 'Preference for higher-rated items despite genre differences']\n",
       " I have a strong preference for rock and alternative music, particularly valuing emotional depth in the songs I listen to. I tend to favor higher-rated items, even if they belong to different genres, and I appreciate music with historical significance. Recently, I interacted with 'Vs.' by Pearl Jam and 'Issues' by Korn, which reflects my inclination towards impactful albums. These patterns suggest that I will continue to seek out emotionally resonant music with high ratings, regardless of genre, in my future choices.,\n",
       " 'AERDZPURXIO2TZ5PTAUPJWJXMBMA': UserAgent(user_id=AERDZPURXIO2TZ5PTAUPJWJXMBMA)\n",
       " Long Term Memory: ['Preference for country and contemporary crossover music', 'Value appreciation for music priced under $10', 'Interest in nostalgic and vibrant music experiences']\n",
       " Short Term Memory: [\"Liked Shania Twain's 'Greatest Hits' for its high rating and value\", \"Disliked P!nk's 'Missundaztood' due to higher price\", 'Preference for upbeat and catchy tunes over emotional rock ballads']\n",
       " I have a strong preference for country and contemporary crossover music, particularly enjoying upbeat and catchy tunes over emotional rock ballads. I value music experiences that are nostalgic and vibrant, and I tend to appreciate items priced under $10. Recently, I liked Shania Twain's 'Greatest Hits' for its high rating and value, while I disliked P!nk's 'Missundaztood' due to its higher price. These patterns suggest that I will likely continue to seek out affordable, high-quality music that aligns with my taste for lively and nostalgic sounds.,\n",
       " 'AFHCS4IBOQ6FSCX2OYE6PWCJOG7Q': UserAgent(user_id=AFHCS4IBOQ6FSCX2OYE6PWCJOG7Q)\n",
       " Long Term Memory: ['Interest in music albums', 'Preference for high average ratings', 'Appeal for nostalgic music experiences']\n",
       " Short Term Memory: [\"Liked 'Very Best Of Prince' album\", \"Disliked 'Significant Other' by Limp Bizkit\", \"Preference for late '90s music nostalgia\"]\n",
       " I have a strong interest in music albums, particularly those with high average ratings, and I am drawn to nostalgic music experiences, especially from the late '90s. Recently, I enjoyed the 'Very Best Of Prince' album, indicating a preference for classic and well-regarded artists. Conversely, I disliked 'Significant Other' by Limp Bizkit, suggesting that I may be selective about the genres and styles I appreciate. These preferences indicate that I am likely to continue seeking out high-quality music from the late '90s that evokes nostalgia.,\n",
       " 'AHWW7ANU7P7LGCBTIO2ACTADQK5A': UserAgent(user_id=AHWW7ANU7P7LGCBTIO2ACTADQK5A)\n",
       " Long Term Memory: ['Preference for punk and alternative music', 'Value high average ratings', 'Consider price and accessibility in choices']\n",
       " Short Term Memory: [\"Recent interaction with 'Warning' by Green Day, rated 4.7\", \"Recent interaction with 'Led Zeppelin IV aka ZOSO', rated 4.8\", 'Preference for classic rock over punk in recent decision']\n",
       " I have a strong preference for punk and alternative music, but I have recently shown a greater inclination towards classic rock, as evidenced by my higher rating for 'Led Zeppelin IV aka ZOSO' compared to 'Warning' by Green Day. I value high average ratings when selecting music and consider both price and accessibility in my choices, which suggests that I will likely continue to prioritize well-rated classic rock albums while remaining open to punk music, provided they meet my standards for quality and value.,\n",
       " 'AGVFLXOWJIHQVUYPNDSKDL4REH4A': UserAgent(user_id=AGVFLXOWJIHQVUYPNDSKDL4REH4A)\n",
       " Long Term Memory: ['Preference for emotional depth in music', 'Interest in indie and alternative rock', 'Value accessibility in pricing']\n",
       " Short Term Memory: [\"Recent interaction with Coldplay's 'A Rush of Blood to the Head'\", \"Preference for Coldplay's emotional storytelling\", 'Comparison of pricing between music albums']\n",
       " I have a strong preference for music that offers emotional depth, particularly within the indie and alternative rock genres. My recent engagement with Coldplay's 'A Rush of Blood to the Head' reflects my appreciation for their emotional storytelling, which resonates with my overall taste in music. Additionally, I value accessibility in pricing when considering music albums, indicating that cost is an important factor in my purchasing decisions. These preferences suggest that I will likely continue to seek out emotionally rich music that aligns with my indie and alternative rock interests while being mindful of pricing.}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2user"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_base_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
